# English below 

# ğŸ“š TÃ¼rkÃ§e Kitap YorumlarÄ± Duygu Analizi (Sentiment Analysis)

Bu proje, makine Ã¶ÄŸrenmesi (ML) ve doÄŸal dil iÅŸleme (NLP) tekniklerini kullanarak TÃ¼rkÃ§e kitap yorumlarÄ±nÄ±n duygu polaritesini (Olumlu/Olumsuz) analiz etmeyi amaÃ§lamaktadÄ±r. Proje kapsamÄ±nda, dengeli iki farklÄ± veri seti (10.402 ve 24.748 yorum) oluÅŸturulmuÅŸ ve bu veri setleri Ã¼zerinde popÃ¼ler sÄ±nÄ±flandÄ±rma algoritmalarÄ±nÄ±n performanslarÄ± karÅŸÄ±laÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r.

## ğŸ¯ Temel AmaÃ§lar

* BÃ¼yÃ¼k bir TÃ¼rkÃ§e yorum veri seti Ã¼zerinde temizleme ve Ã¶n iÅŸleme adÄ±mlarÄ±nÄ± uygulamak.
* Dengeli bir eÄŸitim seti oluÅŸturarak model baÅŸarÄ±sÄ±nÄ± artÄ±rmak.
* CountVectorizer kullanarak metin verisini makine Ã¶ÄŸrenmesi algoritmalarÄ± iÃ§in sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rmek.
* FarklÄ± sÄ±nÄ±flandÄ±rma modellerinin performanslarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rarak en uygun modeli belirlemek.

## âš™ï¸ Uygulanan Metodoloji

### Veri KaynaÄŸÄ± ve Ã–n Ä°ÅŸleme

Proje, **bir kitap alÄ±ÅŸveriÅŸ sitesine ait yaklaÅŸÄ±k 400.000 yorum** iÃ§eren bir veri seti ile baÅŸlamÄ±ÅŸtÄ±r. Veri Ã¶n iÅŸleme ve dengeleme adÄ±mlarÄ± sonrasÄ±nda temel model eÄŸitimi iÃ§in **10.402** ve **24.748** yorum iÃ§eren dengeli veri setleri kullanÄ±lmÄ±ÅŸtÄ±r.

1.  **Veri YÃ¼kleme ve Temizleme:** BaÅŸlangÄ±Ã§ verisi olan `tb1.csv`'deki yinelenen (duplicate) 1.433 adet yorum silinmiÅŸtir.
2.  **Dengeleme Stratejisi (Ä°kili SÄ±nÄ±flandÄ±rma):**
    * **Dataset 1 (10.402 Yorum):** 1 Puan (Olumsuz $\rightarrow 0$) ve 5 Puan (Olumlu $\rightarrow 1$) yorumlar eÅŸitlenmiÅŸtir.
    * **Dataset 2 (24.748 Yorum):** 1 ve 2 Puanlar tek bir Olumsuz $\rightarrow 0$ sÄ±nÄ±fÄ±nda toplanmÄ±ÅŸ, 5 Puanlar ise Olumlu $\rightarrow 1$ sÄ±nÄ±fÄ±nÄ± oluÅŸturmak iÃ§in eÅŸitlenmiÅŸtir.
3.  **Metin Ä°ÅŸleme:** TÃ¼rkÃ§e durak kelimeler (`stopwords`) yorum metinlerinden temizlenmiÅŸtir.
4.  **VektÃ¶rleÅŸtirme:** `CountVectorizer` kullanÄ±larak metinler sayÄ±sal Ã¶zellik matrisine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.

### Makine Ã–ÄŸrenmesi Modelleri

Her iki veri seti (%67 EÄŸitim / %33 Test) Ã¼zerinde aÅŸaÄŸÄ±daki algoritmalar eÄŸitilmiÅŸ ve kaydedilmiÅŸtir (`.model` ve `vectorizer.pickle` dosyalarÄ±):

1.  Lojistik Regresyon (`LogisticRegression`)
2.  K-En YakÄ±n KomÅŸu (`KNeighborsClassifier`)
3.  Karar AÄŸacÄ± (`DecisionTreeClassifier`)
4.  Rastgele Orman (`RandomForestClassifier`)
5.  Destek VektÃ¶r Makineleri (`SVC`)
6.  Multinomial Naive Bayes (`MultinomialNB`)

## ğŸ“Š Model BaÅŸarÄ±larÄ±nÄ±n KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± (Accuracy Score)

| Model | Dataset 1 (10.402 Yorum) BaÅŸarÄ±sÄ± | Dataset 2 (24.748 Yorum) BaÅŸarÄ±sÄ± |
| :--- | :--- | :--- |
| **Multinomial Naive Bayes** | **0.8695** | (Ã‡alÄ±ÅŸmaya eklenmemiÅŸ) |
| **Lojistik Regresyon** | 0.8567 | **0.8603** |
| **Destek VektÃ¶r Makineleri (SVM)** | 0.8485 | 0.8445 |
| **Random Forest** | 0.8375 | 0.8385 |
| **Decision Tree** | 0.7856 | 0.7828 |
| **K-Nearest Neighbors (KNN)** | 0.7098 (k=5) | 0.7191 (k=15) |

**SonuÃ§:** `Multinomial Naive Bayes` ve `Lojistik Regresyon` algoritmalarÄ±, her iki veri seti Ã¼zerinde de **%86'nÄ±n Ã¼zerinde** doÄŸruluk oranÄ± ile en baÅŸarÄ±lÄ± performansÄ± gÃ¶stermiÅŸtir.

## ğŸ™ TeÅŸekkÃ¼rler

Bu veri setinin saÄŸlanmasÄ±ndaki katkÄ±larÄ±ndan dolayÄ± **https://github.com/malibayram** adresine teÅŸekkÃ¼rler.

***

## ğŸ‡¬ğŸ‡§ English Version

# ğŸ“š Turkish Book Review Sentiment Analysis

This project aims to analyze the sentiment polarity (Positive/Negative) of Turkish book reviews using Machine Learning (ML) and Natural Language Processing (NLP) techniques. Two different balanced datasets (10,402 and 24,748 reviews) were created and the performances of popular classification algorithms were compared on these datasets.

## ğŸ¯ Core Objectives

* To apply cleaning and preprocessing steps on a large Turkish review dataset.
* To create a balanced training set to improve model accuracy.
* To convert text data into numerical vectors for ML algorithms using CountVectorizer.
* To compare the performances of different classification models to determine the most suitable model.

## âš™ï¸ Methodology Applied

### Data Source and Preprocessing

The project started with a dataset containing **approximately 400,000 reviews from a book shopping website**. After data preprocessing and balancing, balanced datasets containing **10,402** and **24,748** reviews were used for core model training.

1.  **Data Cleaning:** 1,433 duplicate reviews from the initial `tb1.csv` file were dropped.
2.  **Balancing Strategy (Binary Classification):**
    * **Dataset 1 (10,402 Reviews):** 1-star (Negative $\rightarrow 0$) and 5-star (Positive $\rightarrow 1$) reviews were equalized.
    * **Dataset 2 (24,748 Reviews):** 1 and 2-star reviews (total 12,374) were grouped into the Negative $\rightarrow 0$ class, and 5-star reviews (12,374) were equalized to form the Positive $\rightarrow 1$ class.
3.  **Text Processing:** Turkish stopwords were removed from the review texts.
4.  **Vectorization:** Text data was converted into a numerical feature matrix using `CountVectorizer`.

### Machine Learning Models

The following classification algorithms were trained and saved (`.model` and `vectorizer.pickle` files) on both datasets (67% Train / 33% Test):

1.  Logistic Regression
2.  K-Nearest Neighbors (KNN)
3.  Decision Tree
4.  Random Forest
5.  Support Vector Machine (SVM)
6.  Multinomial Naive Bayes (MNB)

## ğŸ“Š Comparison of Model Accuracies (Accuracy Score)

| Model | Dataset 1 (10,402 Reviews) Accuracy | Dataset 2 (24,748 Reviews) Accuracy |
| :--- | :--- | :--- |
| **Multinomial Naive Bayes** | **0.8695** | (Not included in analysis) |
| **Logistic Regression** | 0.8567 | **0.8603** |
| **Support Vector Machine (SVM)** | 0.8485 | 0.8445 |
| **Random Forest** | 0.8375 | 0.8385 |
| **Decision Tree** | 0.7856 | 0.7828 |
| **K-Nearest Neighbors (KNN)** | 0.7098 (k=5) | 0.7191 (k=15) |

**Conclusion:** `Multinomial Naive Bayes` and `Logistic Regression` algorithms showed the best performance with an accuracy rate of **over 86%** on both datasets.

## ğŸš€ How to Run the Project

1.  Clone the repository:
    ```bash
    git clone [YOUR_REPO_ADDRESS]
    ```
2.  Install the necessary libraries:
    ```bash
    pip install pandas numpy scikit-learn nltk seaborn matplotlib
    ```
3.  Open the Jupyter Notebook file:
    ```bash
    jupyter notebook NLP_Kitap-ML.ipynb
    ```
4.  Run the code cells sequentially to see the creation of the datasets, model training, and testing phases.

## ğŸ™ Acknowledgements

Thanks to **https://github.com/malibayram** for contributing the dataset.
